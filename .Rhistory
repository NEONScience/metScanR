api_token <- "f68fd4db05c32933b94e1760104cfdaa01eda4956c883943ab9fe55f2fb99313"
## intcheck does the following: (1) check and set global LOAD_ID/LOAD_STATUS JSON key, (2) maps the parserPaths, (3) checks the LOV
## (4) writes a new file with parserPaths for each ingest table ("withFulcrumKeys.txt")
ingesttables <- intcheck(input_df = workbookLocations, api_token, ingestWB, path_output, FALSE) # returns a list
## this will give you all of the LOV values listed in the ingest workbook -- currently does not check if LOVs are present in form schema
## the 'valid' column lists what the valid value is based on the Shiny app
## TODO: CURRENTLY DOESNT VERIFY IF THE APP LOV IS CORRECT, JUST LISTS THE LOV
lov_verify <- lovcheck(input_df = workbookLocations, api_token, ingestWB, path_output, FALSE, lov = lov)  # returns a list
# Collapse the list >> combine the separate files into one file
combined <- plyr::ldply(ingesttables)
# this re-combines all of the ingest tables into one file
write.table(combined, file = paste(path_output, "combined_ingest_withKeys.txt", sep = "/"),col.names=T, row.names=F, quote=T, na="",sep="\t",fileEncoding = "UTF-8")
# are any fields missing a parserPath?
missingFields <- dplyr::filter(combined, is.na(parserPath), fieldName != "uid") %>% select(table,fieldName, parserToCreate, entryValidationRulesParser, entryValidationRulesForm, entryLabelIfDifferentFromFieldName,parserPath)
# View it
View(missingFields)
## View the LOV
View(lov_verify[[1]])
# CTRL + ALT + C runs all code within a chunk
library(knitr)
library(kfigr)
library(httr)
library(plyr) # possibly use to cast children$form_values to a single data frame
library(dplyr)
library(tidyjson) # https://github.com/sailthru/tidyjson
library(jsonlite) # https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html
library(rJava)
library(xlsxjars)
#Set working directory and file paths
rm(list=ls())
options(stringsAsFactors = FALSE)
########################## EDIT THESE PATHS FOR YOUR OUTPUT FOLDER AND INPUT INGEST WORKBOOK ##########################
########################## EDIT THESE PATHS FOR YOUR OUTPUT FOLDER AND INPUT INGEST WORKBOOK ##########################
########################## EDIT THESE PATHS FOR YOUR OUTPUT FOLDER AND INPUT INGEST WORKBOOK ##########################
## Specify output file path
## Specify input ingest workbook file
if (file.exists(
"c:/Users/rregan")){
path_output <- "c:/Users/rregan/Desktop/FulcrumGit/devTOS/fulcrum/integration_tests/smammals"
path_devTOS <- "C:/Users/rregan/Desktop/FulcrumGit/devTOS"
}
if (file.exists(
"C:/Users/nrobinson")){
path_output <- "C:/Users/nrobinson/Desktop/MyDocuments/NEON_Git/devTOS/fulcrum/integration_tests/smammals"
path_devTOS <- "C:/Users/nrobinson/Desktop/MyDocuments/NEON_Git/devTOS"
}
if (file.exists(
"C:/Users/cflagg")){
path_output <- "C:/Users/cflagg/Documents/GitHub/devTOS/fulcrum/integration_tests/phenology"
path_devTOS <- "C:/Users/cflagg/Documents/GitHub/devTOS"
path_organismal <- "C:/Users/cflagg/Documents/GitHub/organismalIPT"
}
if (file.exists(
"C:/Users/kcawley")){
path_output <- "C:/Users/kcawley/Documents/GitHub/devTOS/fulcrum/integration_tests/smammals"
path_devTOS <- "C:/Users/kcawley/Documents/GitHub/devTOS"
}
# The ____folder name____ inside integration_tests
ingestWB <- read.delim(paste0(path_organismal,"/phenology/defData/phe_dataingest_NEONDOC001408.txt"), header = TRUE)
############## EDIT THESE CHARACTER VECTORS TO MATCH YOUR INGEST WORKBOOK NAME, FULCRM IDS, AND INGEST TABLE NAMES #######
############## EDIT THESE CHARACTER VECTORS TO MATCH YOUR INGEST WORKBOOK NAME, FULCRM IDS, AND INGEST TABLE NAMES #######
############## EDIT THESE CHARACTER VECTORS TO MATCH YOUR INGEST WORKBOOK NAME, FULCRM IDS, AND INGEST TABLE NAMES #######
###### specify fulcrum form_ids and ingest table names
# this should match the name of the ingest workbook .txt file
wbName <- c(rep("phe_dataingest_NEONDOC001408"))
# this should match the names of the ingest workbook tables
tableName <- c("phe_perindividual_in", "phe_statusintensity_in")
# this should match the fulcrum form_id of the apps that are INGESTED (apps that are not ingested should not be referenced here)
form_ids <- c("283a7c86-e453-438c-b02a-ac2a1ae531e9", "8299f45a-ae62-4084-880c-64fe0dfb03dd")
# this data.frame gets passed to the intcheck() function
workbookLocations <- data.frame(form_ids = c(form_ids),
wbName = wbName,
tableName = tableName)
################# EDIT THESE PATHS TO YOUR devTOS Fulcrum Folder -- source files contain functions #################
# Assign current ingestWB headers
source(paste(path_devTOS, "workbookWorkflow/checkAndPrintWB.R", sep = "/"))
# All API functions
source(paste(path_devTOS, "fulcrum/R_scripts/app_check.R", sep = "/")) ## functions that interact with the API
source(paste(path_devTOS, "fulcrum/R_scripts/integration_check.R", sep = "/")) ## functions that verify ingest workbook data
# check for case that path_output is not defined properly...
if ( ! exists("path_output")){ stop('path_output is not defined')}
# Grab the list of values from Christine's Shiny app -- this gets referenced by a function in 'integration_check'
lov <- read.csv(paste(path_output, "DPS_LOV_list_20170118.171150.csv", sep = "/"),
header = T, stringsAsFactors = F)
### JUST RUN THIS
api_token <- "f68fd4db05c32933b94e1760104cfdaa01eda4956c883943ab9fe55f2fb99313"
## intcheck does the following: (1) check and set global LOAD_ID/LOAD_STATUS JSON key, (2) maps the parserPaths, (3) checks the LOV
## (4) writes a new file with parserPaths for each ingest table ("withFulcrumKeys.txt")
ingesttables <- intcheck(input_df = workbookLocations, api_token, ingestWB, path_output, FALSE) # returns a list
## this will give you all of the LOV values listed in the ingest workbook -- currently does not check if LOVs are present in form schema
## the 'valid' column lists what the valid value is based on the Shiny app
## TODO: CURRENTLY DOESNT VERIFY IF THE APP LOV IS CORRECT, JUST LISTS THE LOV
lov_verify <- lovcheck(input_df = workbookLocations, api_token, ingestWB, path_output, FALSE, lov = lov)  # returns a list
# Collapse the list >> combine the separate files into one file
combined <- plyr::ldply(ingesttables)
# this re-combines all of the ingest tables into one file
write.table(combined, file = paste(path_output, "combined_ingest_withKeys.txt", sep = "/"),col.names=T, row.names=F, quote=T, na="",sep="\t",fileEncoding = "UTF-8")
# are any fields missing a parserPath?
missingFields <- dplyr::filter(combined, is.na(parserPath), fieldName != "uid") %>% select(table,fieldName, parserToCreate, entryValidationRulesParser, entryValidationRulesForm, entryLabelIfDifferentFromFieldName,parserPath)
# View it
View(missingFields)
## View the LOV
View(lov_verify[[1]])
# CTRL + ALT + C runs all code within a chunk
library(knitr)
library(kfigr)
library(httr)
library(plyr) # possibly use to cast children$form_values to a single data frame
library(dplyr)
library(tidyjson) # https://github.com/sailthru/tidyjson
library(jsonlite) # https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html
library(rJava)
library(xlsxjars)
#Set working directory and file paths
rm(list=ls())
options(stringsAsFactors = FALSE)
########################## EDIT THESE PATHS FOR YOUR OUTPUT FOLDER AND INPUT INGEST WORKBOOK ##########################
########################## EDIT THESE PATHS FOR YOUR OUTPUT FOLDER AND INPUT INGEST WORKBOOK ##########################
########################## EDIT THESE PATHS FOR YOUR OUTPUT FOLDER AND INPUT INGEST WORKBOOK ##########################
## Specify output file path
## Specify input ingest workbook file
if (file.exists(
"c:/Users/rregan")){
path_output <- "c:/Users/rregan/Desktop/FulcrumGit/devTOS/fulcrum/integration_tests/smammals"
path_devTOS <- "C:/Users/rregan/Desktop/FulcrumGit/devTOS"
}
if (file.exists(
"C:/Users/nrobinson")){
path_output <- "C:/Users/nrobinson/Desktop/MyDocuments/NEON_Git/devTOS/fulcrum/integration_tests/smammals"
path_devTOS <- "C:/Users/nrobinson/Desktop/MyDocuments/NEON_Git/devTOS"
}
if (file.exists(
"C:/Users/cflagg")){
path_output <- "C:/Users/cflagg/Documents/GitHub/devTOS/fulcrum/integration_tests/phenology"
path_devTOS <- "C:/Users/cflagg/Documents/GitHub/devTOS"
path_organismal <- "C:/Users/cflagg/Documents/GitHub/organismalIPT"
}
if (file.exists(
"C:/Users/kcawley")){
path_output <- "C:/Users/kcawley/Documents/GitHub/devTOS/fulcrum/integration_tests/smammals"
path_devTOS <- "C:/Users/kcawley/Documents/GitHub/devTOS"
}
# The ____folder name____ inside integration_tests
ingestWB <- read.delim(paste0(path_organismal,"/phenology/defData/phe_dataingest_NEONDOC001408.txt"), header = TRUE)
############## EDIT THESE CHARACTER VECTORS TO MATCH YOUR INGEST WORKBOOK NAME, FULCRM IDS, AND INGEST TABLE NAMES #######
############## EDIT THESE CHARACTER VECTORS TO MATCH YOUR INGEST WORKBOOK NAME, FULCRM IDS, AND INGEST TABLE NAMES #######
############## EDIT THESE CHARACTER VECTORS TO MATCH YOUR INGEST WORKBOOK NAME, FULCRM IDS, AND INGEST TABLE NAMES #######
###### specify fulcrum form_ids and ingest table names
# this should match the name of the ingest workbook .txt file
wbName <- c(rep("phe_dataingest_NEONDOC001408"))
# this should match the names of the ingest workbook tables
tableName <- c("phe_perindividual_in", "phe_statusintensity_in")
# this should match the fulcrum form_id of the apps that are INGESTED (apps that are not ingested should not be referenced here)
form_ids <- c("283a7c86-e453-438c-b02a-ac2a1ae531e9", "8299f45a-ae62-4084-880c-64fe0dfb03dd")
# this data.frame gets passed to the intcheck() function
workbookLocations <- data.frame(form_ids = c(form_ids),
wbName = wbName,
tableName = tableName)
################# EDIT THESE PATHS TO YOUR devTOS Fulcrum Folder -- source files contain functions #################
# Assign current ingestWB headers
source(paste(path_devTOS, "workbookWorkflow/checkAndPrintWB.R", sep = "/"))
# All API functions
source(paste(path_devTOS, "fulcrum/R_scripts/app_check.R", sep = "/")) ## functions that interact with the API
source(paste(path_devTOS, "fulcrum/R_scripts/integration_check.R", sep = "/")) ## functions that verify ingest workbook data
# check for case that path_output is not defined properly...
if ( ! exists("path_output")){ stop('path_output is not defined')}
# Grab the list of values from Christine's Shiny app -- this gets referenced by a function in 'integration_check'
lov <- read.csv(paste(path_output, "DPS_LOV_list_20170118.171150.csv", sep = "/"),
header = T, stringsAsFactors = F)
### JUST RUN THIS
api_token <- "f68fd4db05c32933b94e1760104cfdaa01eda4956c883943ab9fe55f2fb99313"
## intcheck does the following: (1) check and set global LOAD_ID/LOAD_STATUS JSON key, (2) maps the parserPaths, (3) checks the LOV
## (4) writes a new file with parserPaths for each ingest table ("withFulcrumKeys.txt")
ingesttables <- intcheck(input_df = workbookLocations, api_token, ingestWB, path_output, FALSE) # returns a list
## this will give you all of the LOV values listed in the ingest workbook -- currently does not check if LOVs are present in form schema
## the 'valid' column lists what the valid value is based on the Shiny app
## TODO: CURRENTLY DOESNT VERIFY IF THE APP LOV IS CORRECT, JUST LISTS THE LOV
lov_verify <- lovcheck(input_df = workbookLocations, api_token, ingestWB, path_output, FALSE, lov = lov)  # returns a list
# Collapse the list >> combine the separate files into one file
combined <- plyr::ldply(ingesttables)
# this re-combines all of the ingest tables into one file
write.table(combined, file = paste(path_output, "combined_ingest_withKeys.txt", sep = "/"),col.names=T, row.names=F, quote=T, na="",sep="\t",fileEncoding = "UTF-8")
# are any fields missing a parserPath?
missingFields <- dplyr::filter(combined, is.na(parserPath), fieldName != "uid") %>% select(table,fieldName, parserToCreate, entryValidationRulesParser, entryValidationRulesForm, entryLabelIfDifferentFromFieldName,parserPath)
# View it
View(missingFields)
## View the LOV
View(lov_verify[[1]])
install.packages("metScanR")
library(metScanR)
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45) # returns 40 stations
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="air temperature") # returns 15 stations
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="air temperature") # returns 15 stations
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="1-minute temperature") # returns 15 stations
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="1-minute air temperature") # returns 15 stations
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="1-minute wind speed") # returns 15 stations
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="air temperature") # returns 15 stations
scenario1
? ls
## quickly extract data from the metScanR DB outputs
ls_getter <- function(input, first, second=NULL){
if (is.null(second)){
lapply(input, "[[", first)
} else {
lapply(lapply(input, "[[", first), "[[", second)
}
}
# extract data from all stations returned
ls_getter(scenario1, "identifiers", "idType")
# collapse into a data frame -- this only works for list elements that have the same dimensions
plyr::ldply(ls_getter(scenario1, "platform"))
## combine uneven vectors
ldply(ls_getter(scenario1, "identifiers", "idType"), rbind)
## base R alternative -- outputs repeat
do.call(rbind, ls_getter(scenario1, "identifiers", "idType"))
plyr::ldply(ls_getter(scenario1, "identifiers", "idType"), rbind)
do.call(rbind, ls_getter(scenario1, "identifiers", "idType"))
do.call(ls_getter(scenario1, "platform"))
do.call(rbind, ls_getter(scenario1, "platform"))
plyr::ldply(ls_getter(scenario1, "platform"))
do.call(rbind, ls_getter(scenario1, "platform"))
ls_getter(scenario1, "identifiers", "idType")
## libraries
library(httr) # talk to the api
library(jsonlite)
library(plyr) # munge lists
library(dplyr) # filter and arrange
## functions
api_token <-  "8141de55fde62012e73c7cf12edda28550172d07f0760eb9141b71bfee1938e5dec475beca94e9ff"
get_prod_forms <- function(api_token = "8141de55fde62012e73c7cf12edda28550172d07f0760eb9141b71bfee1938e5dec475beca94e9ff"){
sql <-  URLencode('SELECT * FROM tables')
url  <-  paste0("https://api.fulcrumapp.com/api/v2/query?token=", api_token, "&format=json", "&q=", sql, "&headers=true")
request <- httr::GET(url, add_headers("X-ApiToken" = api_token, Accept = "application/json"))
content <- jsonlite::fromJSON(httr::content(request, as = "text"))
dat <- content$rows
## give me only PROD apps
prod_forms <- dat %>% dplyr::filter(type %in% c("form", "repeatable"), grepl("PROD", name))
return(prod_forms)
}
get_numRecords <- function(form_id, api_token = "f68fd4db05c32933b94e1760104cfdaa01eda4956c883943ab9fe55f2fb99313"){
sql <- URLencode(paste0('SELECT _record_id FROM "', form_id,'"'))
url <-  paste0("https://api.fulcrumapp.com/api/v2/query?token=", api_token, "&format=json", "&q=", sql, "&headers=true")
request <- httr::GET(url, add_headers("X-ApiToken" = api_token, Accept = "application/json"))
content <- jsonlite::fromJSON(httr::content(request, as = "text"))
num <- nrow(content$rows)
return(num)
}
## Count the number of records per form, only query by a known field, _record_id, to avoid passing massive amounts of data around
## you can pass either the GUID form_id or the literal name of the app to the `form_id` arg
get_records <- function(form_id, api_token){
sql <- URLencode(paste0('SELECT * FROM "', form_id,'"'))
url <-  paste0("https://api.fulcrumapp.com/api/v2/query?token=", api_token, "&format=json", "&q=", sql, "&headers=true")
request <- httr::GET(url, add_headers("X-ApiToken" = api_token, Accept = "application/json"))
content <- jsonlite::fromJSON(httr::content(request, as = "text"))
return(content$rows)
}
# deletion function -- loop through with list of fulcrum_ids
delete_records <- function(record_id, api_token){
require(httr)
url <- paste0("https://api.fulcrumapp.com/api/v2/records/",record_id,".json")
request <- httr::DELETE(url, add_headers("X-ApiToken" = api_token, "Accept" = "application/json"))
return(request)
}
#delete_records("eb3d22d5-347d-4907-aad0-e8c243819ff8", api_token)
# 5000 requests per api_token per hour
# example deletion, where record_list is a *vector* with fulcrum_ids as type *character*
#sapply(X = record_list[,1], FUN = delete_records, api_token = api_token)
get_parent_domain <- function(parent, api_token, domainid){
## regular request
sql = paste(URLencode(paste0('SELECT * FROM "', parent,'" AS parent')),
URLencode(paste0("WHERE domainid LIKE '", domainid, "'")),
sep = "%20")
url =  paste0("https://api.fulcrumapp.com/api/v2/query?token=", api_token, "&format=json", "&q=", sql, "&headers=true")
request = httr::GET(url, add_headers("X-ApiToken" = api_token, Accept = "application/json"))
content = jsonlite::fromJSON(httr::content(request, as = "text"))
out = content$rows
## error handling: if there are no data for a domain, return nothing
if (class(out) == "list" || is.null(out)){
## gimme nothin'
return()
} else {
## gimme somethin'
return(dplyr::select(out,-`_geometry`, -`_created_geometry`, -`_updated_geometry`))
}
}
get_child_domain <- function(parent, child, api_token, domainid){
## regular request
sql = paste(URLencode(paste0('SELECT * FROM "',parent,'" AS parent')),
URLencode(paste0('JOIN "',child,'" AS child')),
URLencode(paste0('ON (parent._record_id = child._record_id)')),
URLencode(paste0("WHERE domainid LIKE '",domainid, "'")),
sep = "%20")
url =  paste0("https://api.fulcrumapp.com/api/v2/query?token=", api_token, "&format=json", "&q=", sql, "&headers=true")
request = httr::GET(url, add_headers("X-ApiToken" = api_token, Accept = "application/json"))
content = jsonlite::fromJSON(httr::content(request, as = "text"))
out = content$rows
## error handling: if there are no data for a domain, return nothing
if (class(out) == "list" || is.null(out)){
## gimme nothin'
return()
} else {
## gimme somethin'
return(dplyr::select(out,-`_geometry`, -`_created_geometry`, -`_updated_geometry`))
}
}
## Identify which form you need
fulcrum_prod <- get_prod_forms()
head(fulcrum_prod)
## reduce to manage
target_forms <- filter(fulcrum_prod, grepl(pattern = "Soil", name))
child_forms <- filter(fulcrum_prod, grepl(pattern="Soil", name), type == "repeatable")
#head(target_forms)
doms <- c("D01", "D02", "D03", "D04", "D05", "D06", "D07", "D08", "D09", "D10", "D11", "D12", "D13", "D14", "D15", "D16", "D17", "D18", "D19", "D20")
slsCore <- lapply(X = doms, FUN = get_child_domain, parent = "Soil Core Collection [PROD]", child = "Soil Core Collection [PROD]/children_soilcores", api_token=api_token)
slsCore <- plyr::ldply(slsCore)
slspH <- get_records((filter(target_forms, grepl("Soil pH", name)) %>% select(form_id)), api_token)
slspH <- select(slspH, -`_geometry`, -`_created_geometry`, -`_updated_geometry`)
slsMoisture <- get_records((filter(target_forms, grepl("Soil Moisture", name)) %>% select(form_id)), api_token)
slsMoisture <- select(slsMoisture, -`_geometry`, -`_created_geometry`, -`_updated_geometry`)
table(filter(slsCore, load_status=="LOADED")$domainid);sum(table(filter(slsCore, load_status=="LOADED")$domainid)) # "records"
filter(slsCore, load_status=="LOADED", domainid=="D14")
filter(slsCore, load_status=="LOADED", domainid=="D14", `_created_at` > "2017-06-01T00:00:00Z")
filter(slsCore, load_status=="LOADED", domainid=="D14", `_created_at` > "2017-06-01T00:00:00")
filter(slsCore, load_status=="LOADED", domainid=="D14", `_created_at` > "2017-06-01")
View(filter(slsCore, load_status=="LOADED", domainid=="D14"))
filter(slsCore, load_status=="LOADED", domainid=="D14", `_created_at` > "2017-06-01T00:00:00.000Z")
filter(slsCore, load_status=="LOADED", domainid=="D14", `_created_at` < "2017-06-01T00:00:00.000Z")
filter(slsCore, load_status=="LOADED", `_created_at` > "2017-06-01T00:00:00.000Z")
filter(slsCore, load_status=="LOADED", `_created_at` > "2017-03-01T00:00:00.000Z")
table(slsCore$load_status)
filter(slspH, load_status=="LOADED", `_created_at` > "2017-03-01T00:00:00.000Z")
filter(slspH, load_status=="LOADED", `_created_at` > "2017-05-01T00:00:00.000Z")
filter(slspH, load_status=="LOADED", `_created_at` > "2017-03-01T00:00:00.000Z")
filter(slsCore, load_status=="LOADED", `_created_at` > "2017-04-01T00:00:00.000Z")
filter(slspH, load_status=="LOADED", `_created_at` > "2017-04-01T00:00:00.000Z", domainid=="D11")
filter(slsCore, load_status=="LOADED", `_created_at` > "2017-04-01T00:00:00.000Z", domainid=="D11")
filter(slsCore, geneticarchivesamplecount=="")
filter(slsCore, is.na(geneticarchivesamplecount))
function
getVars()
getVars()[1]
do.call(rbind, ls_getter(scenario1, "elements"))
ls_getter(scenario1, "identifiers")
ls_getter(scenario1, "elements", "element")
do.call(rbind, ls_getter(scenario1, "elements", "element"))
do.call(rbind, unique(ls_getter(scenario1, "elements", "element")))
unique(do.call(rbind, ls_getter(scenario1, "elements", "element")))
unique(do.call(c, ls_getter(scenario1, "elements", "element")))
ls_getter(scenario1, "identifiers", "idType")
do.call(rbind, ls_getter(scenario1, "identifiers", "idType"))
plyr::ldply(sf_getter(scenario1, "identifiers", "idType"), rbind)
plyr::ldply(ls_getter(scenario1, "identifiers", "idType"), rbind)
oldest <- getDates(startDate = "1800-01-01", endDate="2015-01-01")
oldest <- getDates(startDate = "1800-01-01", endDate="2015-01-01")
oldest
dim(oldest)
str(oldest)
length(oldest)
oldest$USC00226177
mapSiteFinder(getNetwork("SCAN"))
names(oldest)
maps <- mapSiteFinder(getNetwork("SCAN"))
maps
library(metScanR)
scenario1 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45) # returns 40 stations
scenario2 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="air temperature") # returns 15 stations
mapSiteFinder(scenario2)
mapSiteFinder(scenario1)
mapSiteFinder(siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP"))
length(names(siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP")))
mapsiteFinder(getVars("conductivity")
```
mapsiteFinder(getVars("conductivity"))
mapSiteFinder(getVars("conductivity"))
groundwater <- getVars("groundwater")
snow <- getVars("snow depth")
intersect(names(groundwater), names(snow))
groundwater <- getVars("soil moisture")
intersect(names(groundwater), names(snow))
colocated <- intersect(names(soilMoisture), names(snow))
soilMoisture <- getVars("soil moisture")
colocated <- intersect(names(soilMoisture), names(snow))
getStation(colocated)
getStations(colocated)
getId(colocated)
getId(colocated[1:3])
oot <- getId(colocated[1:3])
oot
oot <- getId(colocated[1])
oot
? getId
oot <- getId(names(colocated[1]))
oot
oot <- getId(names(colocated[[1]]))
names(colocated)
colocated
scenario2 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="SNOW",vars="snow") # returns 15 stations
scenario2 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="SNOW",vars="snow depth") # returns 15 stations
scenario2 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="SNOW",vars="snow depth", includeUnk = TRUE) # returns 15 stations
scenario2 <- siteFinder(lat=40.05,lon=-105.27,startDate="2000-01-05",radius=45,network="COOP",vars="air temperature", includeUnk = TRUE) # returns 15 stations
soilMoisture <- getVars("soil moisture")
snow <- getVars("snow depth")
## determine which stations have by finding intersecting station names between the two lists
colocated <- intersect(names(soilMoisture), names(snow))
colocated
## Return the station meta-data
oot <- getId(names(colocated[[1]]))
getId("SNTL:358")
? getId
soilMoisture[[1]]
getId("SCAN:2221")
colocated[[1]]
scenario1[[1]]
names(scenario1[[1]])
names(scenario1)[1]
getId(names(scenario1)[1])
getId("USC00050183")
sapply(colocated, "[[", names(colocated))
sapply(colocated, "[", names(colocated))
sapply(metScanR_DB, "[", colocated)
sapply(metScanR:::metScanR_DB, "[", colocated)
lapply(metScanR:::metScanR_DB, "[", colocated)
metScanR:::metScanR_DB[1]
lapply(metScanR:::metScanR_DB, "[[", colocated[1])
lapply(metScanR:::metScanR_DB, "[", colocated[1])
? lapply
install.packages("purrr")
library(purrr)
metScanR:::metScanR_DB %>%
purrr::map(`[`,colocated)
colocated[1]
metScanR:::metScanR_DB %>%
purrr::map(`[`,colocated[1])
metScanR:::metScanR_DB %>%
purrr::map(`$`,colocated[1])
metScanR:::metScanR_DB$colocated[1]
colocated[1]
metScanR:::metScanR_DB$"SCAN:2221"
metScanR:::metScanR_DB$SCAN:2221
metScanR:::metScanR_DB$"SCAN:2221"
metScanR:::metScanR_DB %>%
purrr::map(`$`,paste0(colocated[1]))
? getNearby
getNearby
? Filter
Filter(function(x) names(x)==colocated[1], metScanR_DB)
Filter(function(x) names(x)==colocated[1], metScanR:::metScanR_DB)
Filter(function(x) match(colocated[1]), metScanR:::metScanR_DB)
? match
collocated %in% metScanR:::metScanR_DB
colocated %in% metScanR:::metScanR_DB
my <- list(a=c(1:3), b=c(1:5), d=c(1:10))
my <- list(a=c(1:3), b=c(1:5), d=c(1:10))
my
x <- c("b", "d")
x
grep(x,lapply(my, "[[", 1))
grep("b",lapply(my, "[[", 1))
grep("d",lapply(my, "[[", 1))
lapply(my, "[[", 1)
lapply(my, "[", 1)
lapply(my, "[", 1)
lapply(names(my), "[", 1)
grep("d",lapply(names(my), "[", 1))
out<-grep("d",lapply(names(my), "[", 1))
my[out]
out<-grep(x,lapply(names(my), "[", 1))
data <- c("b", "d")
lapply(data,function(x) grep(x,lapply(names(my), "[", 1))
my[out]
lapply(data,function(x) grep(x,lapply(names(my), "[", 1)))
out <- lapply(data,function(x) grep(x,lapply(names(my), "[", 1)))
my[out]
out
unlist(out)
my[unlist(out)]
out <- lapply(colocated, function(x) grep(x,lapply(names(metScanR:::metScanR_DB), "[", 1)))
out
metScanR:::metScanR_DB[unlist(out)]
head(ls_getter(scenario1, "elements", "element"))
ls_getter <- function(input, first, second=NULL){
if (is.null(second)){
lapply(input, "[[", first)
} else {
lapply(lapply(input, "[[", first), "[[", second)
}
}
ls_getter(scenario1, "elements", "element") %>% head
unique(do.call(c, ls_getter(scenario1, "elements", "element"))) %>% head
unique(do.call(c, ls_getter(scenario1, "elements", "element")))
unique(do.call(c, ls_getter(scenario1, "elements", "element")))
metScanR:::metScanR_DB[1]
metScanR:::metScanR_DB[[1]]
lapply(colocated[1], function(x) metScanR:::metScanR_DB[x])
st1 <- system.time()
st1
st1 <- Sys.time()
co_index <- lapply(colocated, function(x) grep(x,lapply(names(metScanR:::metScanR_DB), "[", 1)))
ed1 <- Sys.time()
## quicker method
st2 <- Sys.time()
co2 <- lapply(colocated, function(x) metScanR:::metScanR_DB[x])
ed2 <- Sys.time()
ed1-st1
ed2-st2
co2
colocated_metadata %>% head(3)
colocated_metadata <- lapply(colocated, function(x) metScanR:::metScanR_DB[x])
colocated_metadata %>% head(3)
colocated %>% head
1.2*60
72/1.2
